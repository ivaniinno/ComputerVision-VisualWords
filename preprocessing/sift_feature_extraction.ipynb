{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **SIFT Featrure Extraction**\n",
        "\n",
        "#### **Overview**\n",
        "\n",
        "The overall size of the features for the WikiArt dataset is about `240 GB`.  All the computations were performed using several environments according to the following code (responses were not saved):\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "chunk_size = 1000\n",
        "total_images = len(final_df['filename'])\n",
        "sift = cv2.SIFT_create()\n",
        "START = 22000\n",
        "STOP = total_images\n",
        "\n",
        "for chunk_start in tqdm(range(START, STOP, chunk_size), desc='Processing chunks'):\n",
        "    chunk_end = min(chunk_start + chunk_size, total_images)\n",
        "    chunk_descriptors = []\n",
        "    chunk_filenames = []\n",
        "\n",
        "    for i in range(chunk_start, chunk_end):\n",
        "        path = final_df['filename'].iloc[i]\n",
        "        try:\n",
        "            full_path = IMG_PATH + path\n",
        "            img = cv2.imread(full_path)\n",
        "\n",
        "            if img is None:\n",
        "                chunk_descriptors.append(None)\n",
        "                chunk_filenames.append(path)\n",
        "                continue\n",
        "\n",
        "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "            _, descriptors = sift.detectAndCompute(gray, None)\n",
        "\n",
        "            if descriptors is None:\n",
        "                chunk_descriptors.append(None)\n",
        "            else:\n",
        "                chunk_descriptors.append(descriptors)\n",
        "\n",
        "            chunk_filenames.append(path)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'EXCEPTION for {path}:\\n{e}')\n",
        "            chunk_descriptors.append(None)\n",
        "            chunk_filenames.append(path)\n",
        "\n",
        "    np.savez_compressed(f'sift_chunk_{chunk_start}_to_{chunk_end - 1}.npz',\n",
        "                        descriptors_list=np.array(chunk_descriptors, dtype=object),\n",
        "                        filenames=np.array(chunk_filenames))\n",
        "\n",
        "    print(f'SAVED: {chunk_start} to {chunk_end - 1} ({len(chunk_descriptors)} images)')\n",
        "\n",
        "    del chunk_descriptors, chunk_filenames\n",
        "```\n",
        "\n",
        "Then the obtained descriptors were processed as chunks, and then PCA and the quantization were performed:"
      ],
      "metadata": {
        "id": "uhvZDslEKE-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import joblib\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "SuPR-6DVJfvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To reduce the descriptor dimension (`128 -> 64`), it was decided to apply PCA. We have used Incremental PCA to train it on all the chunks with SIFT feature."
      ],
      "metadata": {
        "id": "2hqD6dLPMZNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NPZ_PATH = 'sift_npz/'\n",
        "IPCA_OUT_DIR = 'ipca_model/'\n",
        "IPCA_FILE = os.path.join(IPCA_OUT_DIR, 'ipca_final.pkl')\n",
        "PCA_COMPONENTS = 64\n",
        "IPCA_BATCH = 2000\n",
        "SAMPLE_PER_FILE_FOR_IPCA = 1800\n",
        "\n",
        "os.makedirs(NPZ_PATH, exist_ok=True)\n",
        "os.makedirs(IPCA_OUT_DIR, exist_ok=True)\n",
        "\n",
        "CSV_PATH = 'preprocessed files/dataset.csv'\n",
        "if os.path.exists(CSV_PATH):\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    test_arr = df[df['subset'] == 'test']['filename'].to_numpy()\n",
        "    TEST_SET = set(os.path.basename(str(x)) for x in test_arr)\n",
        "    print(f'Loaded test set from {CSV_PATH}, {len(TEST_SET)} filenames will be excluded from IPCA training.')\n",
        "else:\n",
        "    TEST_SET = set()\n",
        "    print(f'Warning: {CSV_PATH} not found â€” no test exclusion will be applied.')\n",
        "\n",
        "EXCLUDED_IMAGE_COUNT = 0\n",
        "INCLUDED_IMAGE_COUNT = 0\n",
        "\n",
        "\n",
        "def list_npz_files(path):\n",
        "    return sorted([f for f in os.listdir(path)\n",
        "                   if f.lower().endswith('.npz') and os.path.isfile(os.path.join(path, f))])\n",
        "\n",
        "\n",
        "def load_npz_fullpath(fullpath):\n",
        "    d = np.load(fullpath, allow_pickle=True)\n",
        "    return d['descriptors_list'], d['filenames']\n",
        "\n",
        "\n",
        "def read_trained_list(path):\n",
        "    if not os.path.exists(path):\n",
        "        return set()\n",
        "    with open(path, 'r', encoding='utf8') as f:\n",
        "        return set(line.strip() for line in f if line.strip())\n",
        "\n",
        "\n",
        "def gather_sample_for_ipca_from_filepaths(filepaths):\n",
        "    global EXCLUDED_IMAGE_COUNT, INCLUDED_IMAGE_COUNT\n",
        "\n",
        "    for fullpath in filepaths:\n",
        "        descriptors_list, filenames = load_npz_fullpath(fullpath)\n",
        "        # Process each descriptor and filename\n",
        "        for desc, fname in zip(descriptors_list, filenames):\n",
        "            fname_base = os.path.basename(str(fname))\n",
        "            if fname_base in TEST_SET:\n",
        "                # Skip test images\n",
        "                EXCLUDED_IMAGE_COUNT += 1\n",
        "                continue\n",
        "            # Include image for training\n",
        "            INCLUDED_IMAGE_COUNT += 1\n",
        "            if desc is None:\n",
        "                continue\n",
        "            desc = np.asarray(desc, dtype=np.float32)\n",
        "            if desc.size == 0:\n",
        "                continue\n",
        "            n = len(desc)\n",
        "            take = min(SAMPLE_PER_FILE_FOR_IPCA, n)\n",
        "            if n > take:\n",
        "                idx = np.random.choice(n, take, replace=False)\n",
        "                yield desc[idx]\n",
        "            else:\n",
        "                yield desc\n",
        "\n",
        "\n",
        "def ipca_partial_fit_on_files(filepaths, ipca, batch_size=IPCA_BATCH):\n",
        "    buffer = []\n",
        "    buf_count = 0\n",
        "    any_sample = False\n",
        "    for sample in tqdm(gather_sample_for_ipca_from_filepaths(filepaths), desc='partial_fit on files'):\n",
        "        any_sample = True\n",
        "        buffer.append(sample)\n",
        "        buf_count += len(sample)\n",
        "        # When buffer is full, train IPCA\n",
        "        if buf_count >= batch_size:\n",
        "            batch = np.vstack(buffer)\n",
        "            ipca.partial_fit(batch)\n",
        "            buffer = []\n",
        "            buf_count = 0\n",
        "    # Train on remaining data\n",
        "    if buf_count > 0:\n",
        "        batch = np.vstack(buffer)\n",
        "        ipca.partial_fit(batch)\n",
        "    return any_sample\n",
        "\n",
        "\n",
        "def save_ipca_parts(ipca, out_dir=IPCA_OUT_DIR):\n",
        "    joblib.dump(ipca, IPCA_FILE)\n",
        "    np.save(os.path.join(out_dir, 'ipca_components.npy'), ipca.components_.astype(np.float32))\n",
        "    if hasattr(ipca, 'mean_'):\n",
        "        np.save(os.path.join(out_dir, 'ipca_mean.npy'), ipca.mean_.astype(np.float32))\n",
        "    print('Saved IPCA to:', IPCA_FILE)\n",
        "\n",
        "\n",
        "all_files = list_npz_files(NPZ_PATH)\n",
        "if len(all_files) == 0:\n",
        "    print('No .npz files found in', NPZ_PATH)\n",
        "    raise SystemExit(0)\n",
        "\n",
        "all_files_basename = [os.path.basename(p) for p in all_files]\n",
        "new_files = [os.path.join(NPZ_PATH, fname) for fname in all_files_basename]\n",
        "\n",
        "if len(new_files) == 0:\n",
        "    print('No new chunks to train on.')\n",
        "    raise SystemExit(0)\n",
        "\n",
        "if os.path.exists(IPCA_FILE):\n",
        "    print('Loading existing IPCA:', IPCA_FILE)\n",
        "    ipca = joblib.load(IPCA_FILE)\n",
        "else:\n",
        "    print('No existing IPCA found. Creating new IncrementalPCA.')\n",
        "    ipca = IncrementalPCA(n_components=PCA_COMPONENTS)\n",
        "\n",
        "print('Running partial_fit on', len(new_files), 'chunks (excluding test images)...')\n",
        "any_sample = ipca_partial_fit_on_files(new_files, ipca, batch_size=IPCA_BATCH)\n",
        "if not any_sample:\n",
        "    print('Warning: no samples were found in the provided new chunks (after excluding test images).')\n",
        "else:\n",
        "    save_ipca_parts(ipca, IPCA_OUT_DIR)\n",
        "    append_trained_list(TRAINED_LIST, [os.path.basename(p) for p in new_files])\n",
        "    print('Appended', len(new_files), 'filenames to', TRAINED_LIST)\n",
        "\n",
        "print(f'IPCA training skipped {EXCLUDED_IMAGE_COUNT} images because they belong to the test set.')\n",
        "print(f'IPCA training included {INCLUDED_IMAGE_COUNT} images from the processed chunks.')"
      ],
      "metadata": {
        "id": "SMmPrR8Ea9q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training the IPCA, it can be applied to the data. After the dimension reduction, we choose the top `1800` best descriptors according to their `l2` norm:"
      ],
      "metadata": {
        "id": "vgNBkHnTa2Ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NPZ_IN = 'sift_npz/'\n",
        "PCA_OUT = 'sift_pca_selected/'\n",
        "IPCA_FILE = 'ipca_model/ipca_final.pkl'\n",
        "COMPRESSED_SIZE = 1800  # K\n",
        "PCA_COMPONENTS = 64  # D2\n",
        "\n",
        "os.makedirs(PCA_OUT, exist_ok=True)\n",
        "\n",
        "\n",
        "def list_npz(path):\n",
        "    return sorted([os.path.join(path, f) for f in os.listdir(path) if\n",
        "                   f.endswith('.npz') and os.path.isfile(os.path.join(path, f))])\n",
        "\n",
        "\n",
        "def load_chunk(path):\n",
        "    d = np.load(path, allow_pickle=True)\n",
        "    return d['descriptors_list'], d['filenames']\n",
        "\n",
        "\n",
        "def save_pca_selected(out_path, descriptors_list, filenames):\n",
        "    np.savez_compressed(out_path, descriptors_list=np.array(descriptors_list, dtype=object), filenames=filenames)\n",
        "\n",
        "\n",
        "ipca = joblib.load(IPCA_FILE)\n",
        "files = list_npz(NPZ_IN)\n",
        "\n",
        "for fp in tqdm(files, desc='Processing chunks to PCA-selected'):\n",
        "    outp = os.path.join(PCA_OUT, os.path.basename(fp).replace('.npz', '.pca_selected.npz'))\n",
        "    if os.path.exists(outp):\n",
        "        continue\n",
        "    descs, filenames = load_chunk(fp)\n",
        "    descs_sel = []\n",
        "    for desc in descs:\n",
        "        if desc is None:\n",
        "            descs_sel.append(None)\n",
        "            continue\n",
        "        desc = np.asarray(desc, dtype=np.float32)\n",
        "        if desc.size == 0:\n",
        "            descs_sel.append(None)\n",
        "            continue\n",
        "\n",
        "        proj = ipca.transform(desc)  # (N, D2)\n",
        "        norms = np.linalg.norm(proj, axis=1)\n",
        "        k = min(COMPRESSED_SIZE, len(norms))\n",
        "        idx = np.argsort(norms)[-k:]\n",
        "        idx = idx[np.argsort(norms[idx])[::-1]]\n",
        "        proj_sel = proj[idx].astype(np.float32)\n",
        "        descs_sel.append(proj_sel)\n",
        "    save_pca_selected(outp, descs_sel, filenames)\n",
        "\n",
        "print('Done. Selected PCA files in:', PCA_OUT)"
      ],
      "metadata": {
        "id": "Yix2M4oKYsDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To apply the quantization, it is necessary to calculate the global minimum and maximum within all the descriptors. This was also performed chunk by chunk:"
      ],
      "metadata": {
        "id": "TDrb-wJHbaqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PCA_SELECTED_DIR = 'sift_pca_selected/'\n",
        "OUT_SCALE_FILE = 'global_pca_scale.npz'\n",
        "LOW_PCT = 0.1\n",
        "HIGH_PCT = 99.9\n",
        "SAMPLE_PER_IMAGE = 1800\n",
        "MARGIN_REL = 0.01\n",
        "\n",
        "\n",
        "def list_pca_selected_files(folder):\n",
        "    pattern = os.path.join(folder, '*.pca_selected.npz')\n",
        "    files = sorted(glob.glob(pattern))\n",
        "    return files\n",
        "\n",
        "\n",
        "def sample_rows(arr, max_samples):\n",
        "    n = arr.shape[0]\n",
        "    if n <= max_samples:\n",
        "        return arr\n",
        "    idx = np.random.choice(n, max_samples, replace=False)\n",
        "    return arr[idx]\n",
        "\n",
        "\n",
        "def compute_global_percentile_bounds(folder,\n",
        "                                     low_pct=LOW_PCT,\n",
        "                                     high_pct=HIGH_PCT,\n",
        "                                     sample_per_image=SAMPLE_PER_IMAGE):\n",
        "    files = list_pca_selected_files(folder)\n",
        "    if len(files) == 0:\n",
        "        raise FileNotFoundError(f'No .pca_selected.npz files found in '{folder}'')\n",
        "\n",
        "    per_image_lows = []\n",
        "    per_image_highs = []\n",
        "    images_seen = 0\n",
        "    files_seen = 0\n",
        "\n",
        "    for fp in tqdm(files, desc='Files'):\n",
        "        try:\n",
        "            d = np.load(fp, allow_pickle=True)\n",
        "        except Exception as e:\n",
        "            print(f'Warning: failed to load {fp}: {e}')\n",
        "            continue\n",
        "        descs = d.get('descriptors_list', None)\n",
        "        if descs is None:\n",
        "            continue\n",
        "        files_seen += 1\n",
        "        # descs is None or (k, D2) arrays\n",
        "        for arr in descs:\n",
        "            if arr is None:\n",
        "                continue\n",
        "            arr = np.asarray(arr, dtype=np.float32)\n",
        "            if arr.size == 0:\n",
        "                continue\n",
        "            # sample rows\n",
        "            try:\n",
        "                sampled = sample_rows(arr, sample_per_image)\n",
        "            except Exception:\n",
        "                # fallback\n",
        "                sampled = arr\n",
        "            # compute percentiles\n",
        "            # sampled shape (m, D2)\n",
        "            low = np.percentile(sampled, low_pct, axis=0)\n",
        "            high = np.percentile(sampled, high_pct, axis=0)\n",
        "            per_image_lows.append(low)\n",
        "            per_image_highs.append(high)\n",
        "            images_seen += 1\n",
        "\n",
        "    if images_seen == 0:\n",
        "        raise RuntimeError('No descriptors found in any pca_selected files.')\n",
        "\n",
        "    # stack percentiles -> (n_images, D2)\n",
        "    lows_stack = np.vstack(per_image_lows)  # (N_images, D2)\n",
        "    highs_stack = np.vstack(per_image_highs)  # (N_images, D2)\n",
        "\n",
        "    # take min of lows and max of highs\n",
        "    gmin = np.min(lows_stack, axis=0)\n",
        "    gmax = np.max(highs_stack, axis=0)\n",
        "\n",
        "    span = gmax - gmin\n",
        "    span[span < 1e-8] = 1.0\n",
        "\n",
        "    # apply small relative margin\n",
        "    gmin = gmin - span * MARGIN_REL\n",
        "    gmax = gmax + span * MARGIN_REL\n",
        "\n",
        "    return {\n",
        "        'gmin': gmin.astype(np.float32),\n",
        "        'gmax': gmax.astype(np.float32),\n",
        "        'files_seen': files_seen,\n",
        "        'images_seen': images_seen,\n",
        "        'D': gmin.shape[0]\n",
        "    }\n",
        "\n",
        "\n",
        "res = compute_global_percentile_bounds(\n",
        "    PCA_SELECTED_DIR,\n",
        "    low_pct=LOW_PCT,\n",
        "    high_pct=HIGH_PCT,\n",
        "    sample_per_image=SAMPLE_PER_IMAGE\n",
        ")\n",
        "\n",
        "gmin = res['gmin']\n",
        "gmax = res['gmax']\n",
        "np.savez_compressed(OUT_SCALE_FILE, gmin=gmin, gmax=gmax)\n",
        "\n",
        "print('Saved global PCA scale to:', OUT_SCALE_FILE)\n",
        "print('Files processed:', res['files_seen'])\n",
        "print('Images considered:', res['images_seen'])"
      ],
      "metadata": {
        "id": "nHEkXIpYpbKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the quantization and combine all the chunks:"
      ],
      "metadata": {
        "id": "QBbriYKwby4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PCA_SELECTED_DIR = 'sift_pca_selected/'\n",
        "SCALE_FILE = 'global_pca_scale.npz'\n",
        "OUT_NPZ = 'sift_compressed_all_chunks_uint8.npz'\n",
        "\n",
        "\n",
        "def list_pca_selected_files(folder):\n",
        "    pattern = os.path.join(folder, '*.pca_selected.npz')\n",
        "    files = sorted(glob.glob(pattern))\n",
        "    return files\n",
        "\n",
        "\n",
        "def load_scale(scale_path):\n",
        "    d = np.load(scale_path)\n",
        "    gmin = d['gmin'].astype(np.float32)\n",
        "    gmax = d['gmax'].astype(np.float32)\n",
        "    return gmin, gmax\n",
        "\n",
        "\n",
        "def quantize_projected_array(arr_float32, gmin, gmax):\n",
        "    # safety\n",
        "    denom = (gmax - gmin)\n",
        "    # prevent divide by zero\n",
        "    denom_safe = np.where(denom == 0.0, 1.0, denom)\n",
        "    normalized = (arr_float32 - gmin[np.newaxis, :]) / denom_safe[np.newaxis, :]\n",
        "    normalized = np.clip(normalized, 0.0, 1.0)\n",
        "    quant = np.rint(normalized * 255.0).astype(np.uint8)\n",
        "    return quant\n",
        "\n",
        "\n",
        "def process_all_chunks(pca_selected_dir, scale_file, out_npz_path):\n",
        "    files = list_pca_selected_files(pca_selected_dir)\n",
        "    if len(files) == 0:\n",
        "        raise FileNotFoundError(f'No .pca_selected.npz files found in '{pca_selected_dir}'')\n",
        "\n",
        "    gmin, gmax = load_scale(scale_file)\n",
        "    D = gmin.shape[0]\n",
        "    print(f'Loaded scale: PCA dim = {D}. Found {len(files)} chunk files.')\n",
        "\n",
        "    descriptors_agg = []\n",
        "    filenames_agg = []\n",
        "\n",
        "    total_images = 0\n",
        "    total_quantized_desc = 0\n",
        "    skipped_empty = 0\n",
        "\n",
        "    for fp in tqdm(files, desc='Processing chunks'):\n",
        "        d = np.load(fp, allow_pickle=True)\n",
        "        descs = d['descriptors_list']  # None or (k, D)\n",
        "        fnames = d['filenames']  # array-like of filenames\n",
        "\n",
        "        for arr, fname in zip(descs, fnames):\n",
        "            total_images += 1\n",
        "            if arr is None:\n",
        "                descriptors_agg.append(None)\n",
        "                filenames_agg.append(fname)\n",
        "                skipped_empty += 1\n",
        "                continue\n",
        "            a = np.asarray(arr, dtype=np.float32)\n",
        "            if a.size == 0:\n",
        "                descriptors_agg.append(None)\n",
        "                filenames_agg.append(fname)\n",
        "                skipped_empty += 1\n",
        "                continue\n",
        "            if a.ndim != 2 or a.shape[1] != D:\n",
        "                if a.ndim == 1 and a.size == D:\n",
        "                    # single descriptor -> (1,D)\n",
        "                    a = a.reshape(1, D)\n",
        "                else:\n",
        "                    raise RuntimeError(f'Unexpected descriptor shape {a.shape} in file {fp} for image {fname}')\n",
        "            q = quantize_projected_array(a, gmin, gmax)\n",
        "            descriptors_agg.append(q)\n",
        "            filenames_agg.append(fname)\n",
        "            total_quantized_desc += q.shape[0]\n",
        "\n",
        "    print('Aggregated images:', total_images)\n",
        "    print('Skipped (None/empty) images:', skipped_empty)\n",
        "    print('Total quantized descriptors (sum of keypoints kept):', total_quantized_desc)\n",
        "    # save single npz\n",
        "    print('Saving final aggregated npz to:', out_npz_path)\n",
        "    np.savez_compressed(out_npz_path,\n",
        "                        descriptors_list=np.array(descriptors_agg, dtype=object),\n",
        "                        filenames=np.array(filenames_agg, dtype=object))\n",
        "    print('Saved. File size (bytes):', os.path.getsize(out_npz_path))\n",
        "\n",
        "\n",
        "process_all_chunks(PCA_SELECTED_DIR, SCALE_FILE, OUT_NPZ)"
      ],
      "metadata": {
        "id": "M-oqhWrnoIyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the data were processed and compressed (`240 GB -> 7 GB`)."
      ],
      "metadata": {
        "id": "IIECatLecEpm"
      }
    }
  ]
}